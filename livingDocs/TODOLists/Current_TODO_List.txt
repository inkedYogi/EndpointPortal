model_capabilities_metadata = {
    'flash': { # Assuming 'flash' maps to a specific Gemini Flash version like 2.5
        'Base Architecture': 'Gemini Architecture (Google)',
        'Parameters': 'Not publicly disclosed',
        'Knowledge Cutoff': 'January 2025', # Example cutoff
        'Real-time Search': 'Yes - integrated search tool',
        'Context Window': '1,000,000 tokens',
        'Input Modalities': 'Text, Images, Video, Audio',
        'Output Format': 'Text (up to 64,000 tokens)',
        'Processing Speed': 'Variable (depends on "thinking" settings)',
        'Reasoning Control': 'Hybrid reasoning with adjustable "thinking" budget',
        'Special Capabilities': 'Code execution, function calling, structured output',
        'Primary Use Case': 'Balanced reasoning with flexible speed/accuracy tradeoffs'
    },
    'sonar_pro': { # Assuming 'sonar_pro' is a key you might use if integrated
        'Base Architecture': 'Fine-tuned Llama 3.3 70B',
        'Parameters': '70B+ (with additional fine-tuning)',
        'Knowledge Cutoff': 'Continuous via web access',
        'Real-time Search': 'Yes - core design feature with citations',
        'Context Window': '200,000 tokens',
        'Input Modalities': 'Primarily text queries (processes web data)',
        'Output Format': 'Text with inline citations',
        'Processing Speed': 'Up to 1,200 tokens/second (Cerebras-powered)',
        'Reasoning Control': 'Fixed reasoning approach optimized for search',
        'Special Capabilities': 'Enhanced citation density, real-time fact verification',
        'Primary Use Case': 'Real-time fact-grounded research and information synthesis'
    }
    # Add other models from your model_set here 
    # 'gemini_pro': { ... }
    # 'gemini': { ... }
    # 'llama': { ... } 
    # 'grok': { ... }
}
